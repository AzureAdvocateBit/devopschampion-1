<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://jasonhand.github.io/fastpages/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jasonhand.github.io/fastpages/" rel="alternate" type="text/html" /><updated>2020-02-25T18:35:09-06:00</updated><id>https://jasonhand.github.io/fastpages/feed.xml</id><title type="html">Responding To Incidents</title><subtitle>Your systems are down! Customers are calling. Every moment counts. What do you do?</subtitle><entry><title type="html">What is an incident</title><link href="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-Incidents.html" rel="alternate" type="text/html" title="What is an incident" /><published>2020-02-25T18:35:09-06:00</published><updated>2020-02-25T18:35:09-06:00</updated><id>https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-Incidents</id><content type="html" xml:base="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-Incidents.html">&lt;h2 id=&quot;service-disruption&quot;&gt;Service disruption&lt;/h2&gt;

&lt;p&gt;I think we should fist discuss what we mean by incidents so we can come to a general understanding for today’s presentation.&lt;/p&gt;

&lt;p&gt;I am confident everyone would agree that an incident is a “service disruption” - something that is affecting our user’s ability to use the services they have come to rely on.&lt;/p&gt;

&lt;p&gt;Incidents are commonly…&lt;/p&gt;

&lt;h3 id=&quot;feared-and-avoided&quot;&gt;Feared and Avoided&lt;/h3&gt;

&lt;p&gt;In some cases, we downplay the significance of an outage … or worse … intentionally mis-label or not report a disruption in service for fear of reprimand.&lt;/p&gt;

&lt;p&gt;Historically, we have felt that incidents reflected poorly in several areas of our engineering efforts and the organization.&lt;/p&gt;

&lt;p&gt;It is not been until more recently through many of the conversations around devops and site reliability engineering, that we are starting to rethink incidents and now view them as opportunities to learn and improve our systems.&lt;/p&gt;

&lt;p&gt;Incidents can also be…&lt;/p&gt;

&lt;h2 id=&quot;subjective&quot;&gt;Subjective&lt;/h2&gt;

&lt;p&gt;If you ask engineers across different organizations and industries, you will get many different answers about what an incident is.&lt;/p&gt;

&lt;p&gt;Sometimes it is only when a customer is affected.&lt;/p&gt;

&lt;p&gt;Others will label disruptions as incidents even if a customer never experienced a thing.&lt;/p&gt;

&lt;p&gt;Subjectivity is an unfortunate property of incidents in a lot of cases, even when it comes to identifying severity levels.&lt;/p&gt;

&lt;p&gt;The last thing I want to point out regarding incidents, is that most of what we as engineers and technologists do is planned work.&lt;/p&gt;

&lt;p&gt;We spend a lot of time and effort understanding the work in front of us.&lt;/p&gt;

&lt;p&gt;We calculate story points.  We plan sprints. We mostly have a good idea on what we are supposed to be working on.&lt;/p&gt;

&lt;p&gt;So, when an incident occurs, it is disruptive. It is…&lt;/p&gt;

&lt;h3 id=&quot;unplanned-work&quot;&gt;Unplanned Work&lt;/h3&gt;

&lt;p&gt;Often, we view this as a terrible thing, but if you stop and think about it, incidents are really “investments” in supplying the value we are trying to deliver to end users.&lt;/p&gt;</content><author><name></name></author><summary type="html">Service disruption</summary></entry><entry><title type="html">This is the secondary header to my first post</title><link href="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-JasonTest.html" rel="alternate" type="text/html" title="This is the secondary header to my first post" /><published>2020-02-25T18:35:09-06:00</published><updated>2020-02-25T18:35:09-06:00</updated><id>https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-JasonTest</id><content type="html" xml:base="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-JasonTest.html">&lt;p&gt;How does it look:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;twitter: https://twitter.com/jasonhand/status/1230541316116758529&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">How does it look:</summary></entry><entry><title type="html">Lifecycle of an incident</title><link href="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-Responding-To-Incidents.html" rel="alternate" type="text/html" title="Lifecycle of an incident" /><published>2020-02-25T18:35:09-06:00</published><updated>2020-02-25T18:35:09-06:00</updated><id>https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-Responding-To-Incidents</id><content type="html" xml:base="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-Responding-To-Incidents.html">&lt;p&gt;What I’m here to talk about today is what happens once we receive an alert. What is the next part that takes place? And you can see there’s much more to an incident than just the response. We’ll go over remediation a little more towards the end of the presentation. Here are each of the phases of an incident.&lt;/p&gt;

&lt;h3 id=&quot;phase-1-detection&quot;&gt;Phase 1: Detection&lt;/h3&gt;

&lt;p&gt;A problem has been detected through various tooling and practices&lt;/p&gt;

&lt;h3 id=&quot;phase-2-response&quot;&gt;Phase 2: Response&lt;/h3&gt;

&lt;p&gt;A coordinated effort to get the right people and tooling in place to diagnose, theorize, and triage.&lt;/p&gt;

&lt;h3 id=&quot;phase-3-remediation&quot;&gt;Phase 3: Remediation&lt;/h3&gt;

&lt;p&gt;Efforts made to change the system to either restore service or confirm theories.&lt;/p&gt;

&lt;h3 id=&quot;phase-4-analysis&quot;&gt;Phase 4: Analysis&lt;/h3&gt;

&lt;p&gt;Post-incident retrospective exercise to understand the the full the lifecycle of the incident including the human response.&lt;/p&gt;

&lt;h3 id=&quot;phase-5-readiness&quot;&gt;Phase 5: Readiness&lt;/h3&gt;

&lt;p&gt;Implementing knew knowledge and changes to improve and shorten the time and effects of future incidents.&lt;/p&gt;

&lt;h2 id=&quot;response-foundations&quot;&gt;Response Foundations&lt;/h2&gt;

&lt;p&gt;If we are going to talk about the foundations of building reliable systems including the foundations of a good incident response plan, I think the best place to start is by determining “Who is expected to respond to problems?” and “How do let them know?” The best place to start, is to design what is known as a “Roster” or an on-call team. We’ll talk more about rosters in a moment. First, we’ll talk about some of the “Roles” of the people that make up that “Roster”.  And last, we’ll talk about what a rotation is.&lt;/p&gt;

&lt;h3 id=&quot;rostersrolesrotations&quot;&gt;Rosters/Roles/Rotations&lt;/h3&gt;

&lt;h4 id=&quot;roles&quot;&gt;Roles&lt;/h4&gt;

&lt;h5 id=&quot;primary-responder&quot;&gt;Primary Responder&lt;/h5&gt;

&lt;p&gt;The first role we need to talk about is the “Primary Responder” – the Primary “On-call” engineer.&lt;/p&gt;

&lt;h5 id=&quot;secondary-responder&quot;&gt;Secondary Responder&lt;/h5&gt;

&lt;p&gt;Then we have the secondary responder – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes.&lt;/p&gt;

&lt;h5 id=&quot;incident-commander&quot;&gt;Incident Commander&lt;/h5&gt;

&lt;p&gt;Another key role to identify, in many cases, is the incident commander. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts… Ensuring people are not stepping on each other or undoing each other’s work.  It is good to have a central person who can track what is going on and who is doing what.&lt;/p&gt;

&lt;h5 id=&quot;communication-coordinator&quot;&gt;Communication Coordinator&lt;/h5&gt;

&lt;p&gt;The Communication Coordinator is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It’s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done.&lt;/p&gt;

&lt;h5 id=&quot;scribe&quot;&gt;Scribe&lt;/h5&gt;

&lt;p&gt;The scribe’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing?  It is all data to be analyzed – but only if we capture it.&lt;/p&gt;

&lt;h5 id=&quot;subject-matter-experts&quot;&gt;Subject Matter Experts&lt;/h5&gt;

&lt;p&gt;It’s quite common within on-call rosters to identify subject matter experts, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves?&lt;/p&gt;

&lt;h4 id=&quot;rosters&quot;&gt;Rosters&lt;/h4&gt;

&lt;p&gt;Rosters establish a framework around who is on-call at any given point. A roster, or team, is made up of multiple engineers. Rosters can also contain multiple rotations.&lt;/p&gt;

&lt;h4 id=&quot;rotations&quot;&gt;Rotations&lt;/h4&gt;

&lt;p&gt;A rotation is a scheduled shift. Engineers takes the “on-call” responsibility for their own specific recurring rotation. There are several types of shifts that you can create – some more common than others.&lt;/p&gt;

&lt;h5 id=&quot;24-x-7&quot;&gt;24 x 7&lt;/h5&gt;

&lt;p&gt;is a rotation where engineers will be “on-call” for several days in a row. However, most “Elite/High performers” would agree that rotations longer than 3 or 4 days are detrimental to the overall health of engineering staff and therefore the entire system.&lt;/p&gt;

&lt;h5 id=&quot;follow-the-sun&quot;&gt;Follow the Sun&lt;/h5&gt;

&lt;p&gt;Follow the sun shifts are nice for distributed teams. These allow for engineers to schedule their “on-call” shifts only during their normal working office hours. As they end their day and go home, engineers in a different time zone can take over.&lt;/p&gt;

&lt;p&gt;And of course, there are many ways to customize shifts, especially for weekends when engineers need more flexibility. Engineers should be able to hand off the role to someone when personal conflicts arise.&lt;/p&gt;

&lt;h4 id=&quot;incident-tracking&quot;&gt;Incident Tracking&lt;/h4&gt;

&lt;p&gt;Another thing we want to do, to address the challenges of Tailwind Traders and their incident response improvements is we want to be able to track:&lt;/p&gt;

&lt;h5 id=&quot;when-did-we-know-about-this-problem&quot;&gt;When Did We Know About This Problem&lt;/h5&gt;

&lt;p&gt;Is this a new incident? If we are trying to reduce the time it takes to recover from incidents, we will need to start capturing when we are aware of issues.&lt;/p&gt;

&lt;p&gt;Not only that but we want to keep track of:&lt;/p&gt;

&lt;h5 id=&quot;how-did-we-know-about-the-problem&quot;&gt;How Did We Know About the Problem&lt;/h5&gt;

&lt;p&gt;Did our monitoring systems tell us, or did a customer inform us? If we plan to learn and improve, capturing how we knew will uncover improvement opportunities in our monitoring practices.&lt;/p&gt;

&lt;p&gt;If I am just finding out about a problem…&lt;/p&gt;

&lt;h5 id=&quot;am-i-the-first-to-know&quot;&gt;Am I the First to Know&lt;/h5&gt;

&lt;p&gt;Who else is aware?&lt;/p&gt;

&lt;p&gt;And let us say others are aware of the problem…&lt;/p&gt;

&lt;h5 id=&quot;what-if-anything-is-being-done&quot;&gt;What (if anything) is Being Done&lt;/h5&gt;

&lt;p&gt;Is everyone assuming someone else is looking into it?&lt;/p&gt;

&lt;p&gt;And last…&lt;/p&gt;

&lt;h5 id=&quot;how-bad-is-it&quot;&gt;How Bad is It&lt;/h5&gt;

&lt;p&gt;We may not have any notion of severity or impact and there is no place for us to find out how bad the problem really is, and who is affected. These are tough questions to answer if nothing is tracked.&lt;/p&gt;

&lt;h4 id=&quot;communication-channels&quot;&gt;Communication channels&lt;/h4&gt;

&lt;p&gt;To address some of the challenges around how we communicate we also want to find a way to create a unique channel or space for engineers to discuss the details of the incident - a “conversation bridge” in our persistent group chat tool -which for Tailwind Traders is Microsoft Teams.&lt;/p&gt;

&lt;p&gt;We want a channel that is unique to the incident only. We do not want conversations about other engineering efforts.We don’t want conversations about what people are doing for lunch. We ONLY want conversations related to the incident. Because then we can take that text (or data) and analyze later in a Post-incident review.&lt;/p&gt;

&lt;p&gt;Using Logic App to create a workflow for responses (including Azure Boards, Azure Storage, Microsoft Teams)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Task: Using Logic Apps&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;remediation&quot;&gt;Remediation&lt;/h3&gt;

&lt;p&gt;Although thinking of incidents in terms of phases allows for us to shorten each in their own unique ways, responding to and remediating an incident often begin to blur. Especially when actions to mitigate or improve the situation, have the opposite result.&lt;/p&gt;

&lt;p&gt;Now that we’ve covered the foundations of building a good incident response plan, let’s talk about remediation efforts and what that looks like for Tailwind Traders.&lt;/p&gt;

&lt;h4 id=&quot;supplying-context-and-guidance&quot;&gt;Supplying Context and Guidance&lt;/h4&gt;

&lt;p&gt;Where to start? How on-call engineers should escalate incidents? What metrics, tools, links or general resources might be helpful? In a non-prescriptive way, what can be provided to help assist in the efforts to fix the problems.&lt;/p&gt;

&lt;h4 id=&quot;update-stakeholders&quot;&gt;Update stakeholders&lt;/h4&gt;

&lt;p&gt;It’s important that internal teams are aware of what’s happening when an incident occurs. If we don’t provide consistent updates, stakeholders will start coming around and asking.They have every right to this information, but we’ve got to find a better way to make them aware of an issue and what is being done about it. We need to find a way to be clear about our acknowledgment to our internal teams.&lt;/p&gt;

&lt;h4 id=&quot;clear-expectations&quot;&gt;Clear expectations&lt;/h4&gt;

&lt;p&gt;We should be clear in presenting what we know, what is being done and what kind of expectations should they have in terms of when they’re going to hear back from us?&lt;/p&gt;

&lt;h4 id=&quot;chatops&quot;&gt;ChatOps&lt;/h4&gt;

&lt;p&gt;Task: Webhook (started from Microsoft Teams) hits an Azure Function endpoint to update a static HTML (hypertext markup language) (status) page stored in Azure Storage.&lt;/p&gt;</content><author><name></name></author><summary type="html">What I’m here to talk about today is what happens once we receive an alert. What is the next part that takes place? And you can see there’s much more to an incident than just the response. We’ll go over remediation a little more towards the end of the presentation. Here are each of the phases of an incident.</summary></entry><entry><title type="html">Time to Recover/Remediate/Restore (TTR)</title><link href="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-TTR.html" rel="alternate" type="text/html" title="Time to Recover/Remediate/Restore (TTR)" /><published>2020-02-25T18:35:09-06:00</published><updated>2020-02-25T18:35:09-06:00</updated><id>https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-TTR</id><content type="html" xml:base="https://jasonhand.github.io/fastpages/2020/02/25/2020-02-25-TTR.html">&lt;p&gt;How many of you are familiar with the acronym T. T. R.?&lt;/p&gt;

&lt;p&gt;It is known as the “Time to Recover” or also sometimes referred to as the “time to remediate” or “time to restore.”&lt;/p&gt;

&lt;p&gt;In other words, the total time that it takes for us to bring services back online… with regards to the value we provide our end users and customers. What is the duration?&lt;/p&gt;

&lt;p&gt;This is just one metric that is often measured to understand how teams are performing when it comes to responding to incidents.&lt;/p&gt;

&lt;p&gt;I think you would all agree that complete prevention of problems is not very realistic.&lt;/p&gt;

&lt;p&gt;We must do better at knowing when something is wrong and being able to respond to it in an effective way.&lt;/p&gt;</content><author><name></name></author><summary type="html">How many of you are familiar with the acronym T. T. R.?</summary></entry></feed>