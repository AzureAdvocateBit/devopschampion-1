
var documents = [{
    "id": 0,
    "url": "http://devopschampion.com/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://devopschampion.com/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 2,
    "url": "http://devopschampion.com/2020/03/05/2020-03-05-Subject-Matter-Experts.html",
    "title": "Subject Matter Experts",
    "body": "2020/03/05 - It’s quite common within on-call rosters to identify subject matter experts, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves? Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Building On-call Rosters "
    }, {
    "id": 3,
    "url": "http://devopschampion.com/2020/03/05/2020-03-05-Secondary-Responder.html",
    "title": "Secondary Responder",
    "body": "2020/03/05 - Then we have the secondary responder – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Incident Commander "
    }, {
    "id": 4,
    "url": "http://devopschampion.com/2020/03/05/2020-03-05-Scribe.html",
    "title": "Scribe",
    "body": "2020/03/05 - The scribe’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing? It is all data to be analyzed – but only if we capture it. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Subject Matter Experts "
    }, {
    "id": 5,
    "url": "http://devopschampion.com/2020/03/05/2020-03-05-Primary-Responder.html",
    "title": "Primary Responder",
    "body": "2020/03/05 - The first role we need to talk about is the “Primary Responder” – the Primary “On-call” engineer. This person is expected to acknowledge their awareness of an incident once the alert has been received. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: The Secondary Responder "
    }, {
    "id": 6,
    "url": "http://devopschampion.com/2020/03/05/2020-03-05-Incident-Commander.html",
    "title": "Incident Commander",
    "body": "2020/03/05 - Another key role to identify, in many cases, is the incident commander. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts… Ensuring people are not stepping on each other or undoing each other’s work.  It is good to have a central person who can track what is going on and who is doing what. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: The Communication Coordinator "
    }, {
    "id": 7,
    "url": "http://devopschampion.com/2020/03/05/2020-03-05-Communication-Coordinator.html",
    "title": "Communication Coordinator",
    "body": "2020/03/05 - The Communication Coordinator is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It’s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: The Scribe "
    }, {
    "id": 8,
    "url": "http://devopschampion.com/2020/03/05/2020-03-04-Response.html",
    "title": "Response",
    "body": "2020/03/05 - Once our detection efforts have been configured to send actionable alerts to the people who build the systems, we need to make sure they are sending those alerts to the right people. Right People: How do you know who the right people are? In most cases it is situational. A few things that can be done to help establish some formatlity and standard around responding to incidents is through the use of roles, rosters, and rotations. We’ll go more in depth on what each of those are soon. Tooling: The right person for the job needs the right tools for the job. If someone is responding to an issue they need to get busy immediately. Making sure the right monitoring, communications, access, and documentation is provided is also important. People should be familiar with the tooling and know how and where to find additional resources to help diagnose, theorize, and triage. Diagnose: Everyone experiences problems. Sometimes routinely throughout the day in fact. When something doesn’t go as expected or breaks entirely our impulse is to fix it. In order to do so we must first have a look at what’s currently observable. What is the status? Who and what is impacted? What hints or clues are there? What information do we have to work with? What do we know right now? Theorize: Once information has been obtained, we begin to theorize next best steps. What action can we take to minimize or stop the impact? What are the repurcussions of that action? Will something else go wrong? If we take one action, what result do we expect? In very brief moments we are creatively thinking through as many possible scenarios to restore service as we can. And then stack ranking them based on our own calculations on the probability of success. Triage: At some point we all need help. That could be access to an admin account, theories from subject matter experts, someone to amplify updates to a broader audience. Rarely are incidents viewed as a success if only a single person was involved. Regardless of the size of your response team, by isolating it as a phase in the incident lifecycle, we can examine this section of the timeline for improvements on how we coordinate our response. If it took an excessive amount of time for the engineering team to correct the payment processor problem simply because it took too long to find the right person, with the right tool, and with the appropriate level of access then there are some clear opportunities for improvement right there. Resources: MS Learn: Improve Incident Response with Alerting in Azure Remediation Phase of an Incident "
    }, {
    "id": 9,
    "url": "http://devopschampion.com/2020/03/05/2020-03-04-Remediation.html",
    "title": "Remediation",
    "body": "2020/03/05 - The remediation phase is the blurriest of them all. A big reason is that sometimes there’s no difference between what takes place during the response and an action intended to improve the situation (i. e. remediation step). Much of incident response is just trial and error, quite honestly. We quickly think through what to do, we do it, we hope for quick feedback, we examine if things improved, and we iterate. Because of this, measuring the remediation phase is a bit trickier. What we are looking for is to determine the distinction between when we have identified the fix (or series of fixes) that will result in recovery of services and the end of the incident. What is the time between? Good or bad, it’s data to potentially discuss and discover revelations. It’s not until the analysis phase that engineers can definitively determine the exact point along the incident timeline that everyone agrees the problem and solution were both understood. This underlines the importance of not only capturing the timeline of events, including conversations and actions taken but also analyzing it in retrospect with a diverse audience encouraged to ask questions. Questions that help radiate a broader and more informed knowledge base across an organization. One reason for measuring this way is to set aside the time between definitively knowing what will restore service and when services were actually back. Let’s say the payment process thing from before was pretty easy to determine. It probably took less than 5 minutes to know it was something with the backend talking to an API and that after someone followed a specific series of steps everything would be fine. However, the process to do this is not only complicated and requires administrative access, it’s not well documented, and what is documented is extremely dated. If this type of problem occurs again, we could shorten the total time of the incident, and therefore cost to the business, simply by making a few small adjustments. These types of opportunities begin to surface when we can ask questions like, “how long does the backup script take?”. “Was the documentation helpful?” Just because you can figure out what the problem is in an acceptable amount of time, does not mean your system will recover as quickly as the business needs it to. Once service is restored and things return to normal it’s important to set aside time to reflect on what took place, discuss it openly, broadcast what has been learned, and prepare for the future. This takes us to our next phase of the incident lifecycle - analysis. Resources: MS Learn: Improve Incident Response with Alerting in Azure Analysis Phase of an Incident "
    }, {
    "id": 10,
    "url": "http://devopschampion.com/2020/03/05/2020-03-04-Readiness.html",
    "title": "Readiness",
    "body": "2020/03/05 - During and after a post-incident review many ideas will surface around how to improve not only various aspects of each phase of the lifecycle but also how the team can improve in other areas. Communication for example. During the review, engineers might have pointed out that there were long gaps in the conversation timeline where nobody said anything. It’s helpful to be verbose in what engineers are doing, thinking, even feeling. If someone isn’t completely comfortable following the steps from documentation, we should address that. Who else on the team carries fears about performing actions on the system during an incident? We want our team to be confident and ready. So, what did we learn that helps improve that readiness? Action items aren’t really the point of a post-incident reivew but inevitably, creative ideas will emerge. Some engineering efforts will make sense to schedule and implement as a result of the conversations. Adding telemetry to help keep a better eye on the credit card processing system, for example. Product and engineering teams should work together to prioritize and schedule work for those enhancements. Tradeoffs will be made since the uptime of a new feature is just as important as the feature itself. Ultimately, what’s best for the business is what leadership will have to wrestle with. The bigger win that helps with our readiness efforts is that we have measurements by which we can set goals around. Resources: MS Learn: Improve Incident Response with Alerting in Azure Understanding the Foundations of Incident Response "
    }, {
    "id": 11,
    "url": "http://devopschampion.com/2020/03/05/2020-03-04-Detection.html",
    "title": "Detection",
    "body": "2020/03/05 - The methods used to determine when we have a problem have changed over the years. Alerting a person to a spike in CPU usage isn’t as valuable these days. Especially those in the process of adopting the cloud. Instead, we want to know when our customer is experiencing a problem while using our system. The problems will vary but the methods used to determine when a human needs to get involved have evolved. By monitoring systems in a way that matches the customer’s perspective we can see when they experience a problem rather than we think we experienced a problem. If the customer is experiencing an issue, that’s far more important to the business than any spike in CPU usage. In today’s connected world, no matter how complex or simple a system appears to be there is much more that goes in to what a user experiences. It’s entirely possible that all systems appear healthy and no alarms are going off when in reality users aren’t able to complete a shopping purchase due to a third party payment processor. No amount of monitoring for memory or network performance would have tipped off engineers or leadership to this business impacting problem. Every system is different and there are certainly still legitimate reasons to set up alerts for problems at the component level. However, by and large if we are planning to get engineers involved (especially outside of office hours) then we need to make sure the problem is real, it’s impacting the business, and it requires human intervention immediately. If an alert isn’t actionable - meaning it requires a person or group of people to respond and investigate right away then it’s not an incident. If we can measure some minor details about when amd how we detect problems in the first place then we can look for opportunities to improve. In conversations about what took place with the payment processor incident it is reasonable to ask “how could we have detected this sooner?”. “How could we have detected this … at all?” may be a better question. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Response Phase of an Incident "
    }, {
    "id": 12,
    "url": "http://devopschampion.com/2020/03/05/2020-03-04-Analysis.html",
    "title": "Analysis",
    "body": "2020/03/05 - The post-incident review is where the idea of incidents begin to shift from things that are feared and avoided to things that can provide valuable information to a team and business. Rarely will you find a business today that doesn’t heavily rely on digital services to earn and keep customers. There are going to be problems along the way. Not only that, but customers expect improvements, technology changes, competitors get smarter. There are fewer and fewer industries that can maintain the status quo and continue to exist, let alone prosper. The analysis phase allows for an honest and open retrospective discussion about what took place. We as an organization want to understand the realities of the scenario from an objective perspective. Exactly how you conduct the exercises will vary but the focus of the conversation is on what and how things happened rather than who and why. By identifying the incident timeline as well as the specific highlights, people can identify the beginning and end of each phase, including the conversations that took place. This helps isolate specific areas of improvements such as moving away from using email distribution lists as the default channel and method of delivering actionable alerts. In discussions about the detection phase, it seemed clear to everyone that the problem could have been solved sooner had they known about it sooner. Sometimes even small changes can have a huge impact on improving the overall time to recover. Regardless of how businesses choose to perform their post-incident review, they should take place no more than 36-48 hours after the incident has concluded. We are looking to collect as much objective data as well as testimonial from a diverse set of perspectives. It’s difficult to remember what took place in much detail after a couple of days. If possible, the exercise should be facilitated by someone that was not involved in the incident. Someone who can remove themselves from the timeline of events and perspectives of what took place. Objective data is often easier to obtain when someone else asks the questions and encourages an honest conversation. The point of a post-incident review isn’t to end up with a document or artifact that the meeting took place. It’s not an exploration on what definitively caused the issue in the first place. Our systems are always changing. That’s just where we are today. Businesses that are providing some kind of service to their customers where technology is involved are required to make constant changes. It doesn’t matter if it’s in the cloud, data center, or closet, servers need improvements and replacements. Operating systems need patches and upgrades. Applications need to be updated and restarted. Databases and logs are changing and growing. Networks are coming and going. There’s a lot going on and it’s often difficult to put our thumb on exactly what’s causing what. The good news is, it’s ok. Part of the advantage of examining incidents in phases means that regardless to the problems we experience in the future, we know that we can detect a problem, get the right people involved, and recover services faster than before. We are prepared for the infinite world of possibilities we like to call the “unknown unknowns”. This takes us to our final phase of the incident lifecycle, readiness. Resources: MS Learn: Improve Incident Response with Alerting in Azure Readiness Phase of an Incident "
    }, {
    "id": 13,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Updating-Stakeholders.html",
    "title": "Updating Stakeholders",
    "body": "2020/03/05 - It’s important that internal teams are aware of what’s happening when an incident occurs. If we don’t provide consistent updates, stakeholders will start coming around and asking. They have every right to this information, but we’ve got to find a better way to make them aware of an issue and what is being done about it. We need to find a way to be clear about our acknowledgment to our internal teams. Clear expectations: We should be clear in presenting what we know, what is being done and what kind of expectations should they have in terms of when they’re going to hear back from us? Resources: Azure Functions OverviewMS Learn: Free Azure Function courses Next steps: More to come "
    }, {
    "id": 14,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Understanding-the-Full-Lifecycle-of-an-Incident.html",
    "title": "Understanding the Full Lifecycle of an Incident",
    "body": "2020/03/05 - If we start to think of incidents as a normal part of a system, then we can also build some formality around the patterns and practices we inevitably see when people instinctually do what they do when something goes wrong. From the beginning of a problem to analyzing what and how things happened, we can measure them independantly of each other. By doing so, we can look for improvements in each phase. For example, monitoring systems may be working as expected but because an alert was sent to a email distribution group, once people were aware of the alert, most assumed someone else was investigating the problem. The problem persisted for hours. An incident can be divided into 5 phases. Detection, response, remediation, analysis, and readiness.  Phase 1 - Detection:A problem has been detected through various tooling and practices Phase 2 - Response:A coordinated effort to get the right people and tooling in place to diagnose, theorize, and triage. Phase 3 - Remediation:Efforts made to change the system to either restore service or confirm theories. Phase 4 - Analysis:Post-incident retrospective exercise to understand the the full the lifecycle of the incident including the human response. Phase 5 - Readiness:Implementing knew knowledge and changes to improve and shorten the time and effects of future incidents. Let’s touch on the detection phase just a little more in depth. This is often the best place to start improving your incident response practices. Solid monitoring is the foundation of building reliable systems. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: The Detection Phase of an Incident "
    }, {
    "id": 15,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Understanding-The-Foundations-of-Incident-Response.html",
    "title": "Understanding the Foundations of Incident Response",
    "body": "2020/03/05 - The foundations of building reliable systems including a good incident response plan, have to start with determining “Who is expected to respond to problems?” and “How do let them know?”. The best place to start, is to design what is to establish roles, rosters, and rotations. Roles : Well defined responsibilities and expectations of individuals on an on-call team (or roster). The Primary Responder, for example. Rosters : A group of individuals, each with their own assigned role and understood responsibilities and expectations. The mobile “on-call” team, consisting of multiple members each with thier own assigned role. Rotations :A scheduled shift for individuals where they are “on-call” for a defined period of time. A 24 x n rotation where someone is the** Primary Responder**, for example. It’s worth looking at each of these groups a little closer, so let’s do that now. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Establishing Oncall Roles "
    }, {
    "id": 16,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Tracking-Incident-Details.html",
    "title": "Tracking Incident Details",
    "body": "2020/03/05 - Another thing we want to do, to address the challenges of Tailwind Traders and their incident response improvements is we want to be able to track: When Did We Know About This Problem: Is this a new incident? If we are trying to reduce the time it takes to recover from incidents, we will need to start capturing when we are aware of issues. Not only that but we want to keep track of: How Did We Know About the Problem: Did our monitoring systems tell us, or did a customer inform us? If we plan to learn and improve, capturing how we knew will uncover improvement opportunities in our monitoring practices. If I am just finding out about a problem… Am I the First to Know: Who else is aware? And let us say others are aware of the problem… What (if anything) is Being Done: Is everyone assuming someone else is looking into it? And last… How Bad is It: We may not have any notion of severity or impact and there is no place for us to find out how bad the problem really is, and who is affected. These are tough questions to answer if nothing is tracked. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Measuring Incident Response "
    }, {
    "id": 17,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Supplying-Context-And-Guidance.html",
    "title": "Supplying Context and Guidance",
    "body": "2020/03/05 - Where to start? How on-call engineers should escalate incidents? What metrics, tools, links or general resources might be helpful? In a non-prescriptive way, what can be provided to help assist in the efforts to fix the problems. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Update Stakeholders "
    }, {
    "id": 18,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Remediation-Of-Incidents.html",
    "title": "Remediation Of Incidents",
    "body": "2020/03/05 - Although thinking of incidents in terms of phases allows for us to shorten each in their own unique ways, responding to and remediating an incident often begin to blur. Especially when actions to mitigate or improve the situation, have the opposite result. Now that we’ve covered the foundations of building a good incident response plan, let’s talk about remediation efforts and what that looks like for Tailwind Traders. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Supplying Context &amp; Guidance "
    }, {
    "id": 19,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Measuring-Incident-Response.html",
    "title": "Measuring Incident Response",
    "body": "2020/03/05 - The systems we work in eventually have problems. They are built, maintained, and supported by technolgists such as yourself. And when an issue inevitably occurs, someone needs to take action to restore services. Many organizations don’t currently have an incident response plan in place. In fact, efforts to recover from service disruptions rarely follow any kind of repeatable and measured framework at all. Engineers react rather than respond. With the increased reliance on digital services and their underlying technology it’s more important than ever to establish an explicit response plan. There are small steps that you could take immediately so that when the next problem occurs, everyone knows what to do. The incident itself can be viewed not just as an outage but an opportunity to learn. TTR: Are you familiar with the acronym TTR? It is known as the “Time to Recover” or also sometimes referred to as the “time to remediate” or “time to restore. ” In other words, the total time that it takes for engineers to bring services back online… with regards to the value provided to end users and customers. It is the total duration of time for the incident? The time to recover will vary from incident to incident and the circumstances around what contributed to the problem will rarely repeat. Because of this, measuring the TTR in aggregate can be a misleading metric. The Mean Time to Recover does not reflect a valuable performance metric on either the uptime of your systems or how well (or poorly) teams can respond to and remediate service disruptions. While not a perfect metric, it is one in which organizations can begin to measure how teams are performing when it comes to responding to incidents individually. We will need to examine the entire incident timeline to gain a broader understanding of what took place and where improvements can be made. Most anyone who works with technology will agree that the complete prevention of problems is not very realistic. Instead, we must do better at knowing when something is wrong and being able to respond to it in an effective way. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Establishing On-call Roles "
    }, {
    "id": 20,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Managing-Tasks-from-Group-Chat-ChatOps.html",
    "title": "Managing Tasks from Group Chat - ChatOps",
    "body": "2020/03/05 - Task: Webhook (started from Microsoft Teams) hits an Azure Function endpoint to update a static HTML (hypertext markup language) (status) page stored in Azure Storage.       Previous   Next   "
    }, {
    "id": 21,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Establishing-Oncall-Roles.html",
    "title": "Establishing On-call Roles",
    "body": "2020/03/05 - Creating a repeatable response plan means establishing who does what when something goes wrong. We don’t want there to be any question around who is supposed to be doing what. Because of this, it is important to establish roles and the associated expectations. This isn’t a separation of duties exercise. In fact, we want to encourage less of that. It is however, a way of establishing better coordination and communication. It prevents people from stepping on each others toes while enabling cross-collaboration amongst not only on-call rosters, but an entire organization. Primary Responder: The first role we need to talk about is the “Primary Responder” – the Primary “On-call” engineer. This person is expected to acknowledge their awareness of an incident once the alert has been received. Secondary Responder: Then we have the secondary responder – who is there as back up -Another engineer who can step in if the primary responder is unavailable or unreachable. Or if they just need another pair of eyes. Incident Commander: Another key role to identify, in many cases, is the incident commander. An incident commander can be extremely helpful when you have got a large-scale outage that effects a lot of different components or requires coordination across many teams and different systems. They are great for making sure that engineers stay focused and they are working on their own remediation efforts… Ensuring people are not stepping on each other or undoing each other’s work.  It is good to have a central person who can track what is going on and who is doing what. Communication Coordinator: The Communication Coordinator is meant to be the person working in conjunction with the incident commander to share more information beyond those who are in the firefight actively working to recover from the incident itself. That could be customers. It could be the sales and marketing teams. Maybe your customer support. There are many people within an organization who need to be made aware of what’s taking place and the status around how things are progressing. It’s always good to put someone in charge of managing that communication and making sure that other stakeholders are aware of what is happening and what’s being done. Scribe: The scribe’s role is to document the conversation in as much detail as possible. Teams commonly use phone bridges, conference calls, or video chat to get everyone together and try to understand what is going on, which can certainly help create space for the conversation. However, it is difficult for us to go through and understand in detail what the engineers were saying and doing unless it is transcribed. As a result, a scribe is that person who can help us document as much as possible to review later. What were people saying, doing, feeling, and even experiencing? It is all data to be analyzed – but only if we capture it. Subject Matter Experts: It’s quite common within on-call rosters to identify subject matter experts, so that early responders know who to escalate too quickly. These people should not be on call all the time, of course, but we do want to be able to identify who is our database expert. Who is our front-end expert? Who are the people that we can reach out to if our primary and secondary responders are not able to diagnose and resolve the issue themselves? Let’s take a closer look at each of these roles to better understand their place within our incident response efforts. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: The Primary Responder "
    }, {
    "id": 22,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Establishing-Communications-Channel.html",
    "title": "Establishing Communication Channels",
    "body": "2020/03/05 - To address some of the challenges around how we communicate we also want to find a way to create a unique channel or space for engineers to discuss the details of the incident - a “conversation bridge” in our persistent group chat tool -which for Tailwind Traders is Microsoft Teams. We want a channel that is unique to the incident only. We do not want conversations about other engineering efforts. We don’t want conversations about what people are doing for lunch. We ONLY want conversations related to the incident. Because then we can take that text (or data) and analyze later in a Post-incident review. Using Logic App to create a workflow for responses (including Azure Boards, Azure Storage, Microsoft Teams)  Task: Using Logic Apps       Previous   Next   "
    }, {
    "id": 23,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Determining-What-Incidents-Are.html",
    "title": "Determining What Incidents Are",
    "body": "2020/03/05 - If you search online for “Incident Response” a majority of what you’ll find is information related to security threats and breaches. What doesn’t show up in the results is stuff about how to properly respond to threats related to something else entirely. How should a business respond to technical challenges and failures as they come up? The ones that affect reliability concerns such as availability, latency, correctness, and others. What happens when service level expectations are breached and it’s time for a human to get involved? Services such as VictorOps, PagerDuty, and others provide “on-call” solutions as well as documentation and best practices regarding this type of incident management. Service Now has opinions as well but the language is aimed more for those who follow ITSM guidance regarding service management. Ticketing with a tiered support structure doesn’t provide the fasted path to uptime for many companies however. In the devops and web operations space, the idea of anyone but the engineers building the system responding to customer impacting problems is completely unacceptable. Irresponsible even. Time is of the essence and those who helped build the applications and underlying infrastructure are the best suited to maintain it’s health and upgrades in a production environment. Exactly when an engineer should be expected to take action is why we need to define what we mean by an incident. We can all agree that an incident is a “service disruption” - something that is affecting our user’s ability to use the services they have come to rely on. That’s a given. However, there are other things about incidents that are often overlooked or never considered. For example incidents are commonly subjective, feared, and unexpected Subjective: If you ask engineers across different organizations and industries, you will get many different answers about what an incident is. Sometimes it is only when a customer is affected. Others will label disruptions as incidents even if a customer never experienced a thing. Subjectivity is an unfortunate property of incidents in a lot of cases, even when it comes to identifying severity levels. Feared: In some cases, we downplay the significance of an outage … or worse … intentionally mis-label or not report a disruption in service for fear of reprimand. Historically, we have felt that incidents reflected poorly in several areas of our engineering efforts and the organization. It has not been until more recently through many of the conversations around devops and site reliability engineering, that we are starting to rethink incidents and now view them as opportunities to learn and improve our systems. Unexpected: In other words … unplanned Work Most of what we as engineers and technologists do is planned work. We spend a lot of time and effort understanding the work in front of us. We calculate story points.  We plan sprints. We have a pretty good idea on what we are supposed to be working on. So, when an incident occurs, it is disruptive. It is… unplanned work. Often, we view this as a terrible thing, but in reality, incidents are actually “investments” in supplying the value we are trying to deliver to end users. We just need to change how we look at incidents. Next we’ll discuss the full lifecycle of an incident. From detection to analysis and everything in between. When we start to view incidents through a new lense and begin analyzing for opportunities to learn, you’ll begin to see your systems for what they are and why having a response plan in place makes sense. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Understanding the Full Lifecycle of An Incident "
    }, {
    "id": 24,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Building-Oncall-Rotations.html",
    "title": "Building On-call Rotations",
    "body": "2020/03/05 - A rotation is a scheduled shift. Engineers takes the “on-call” responsibility for their own specific recurring rotation. There are several types of shifts that you can create – some more common than others. 24 x 7: is a rotation where engineers will be “on-call” for several days in a row. However, most “Elite/High performers” would agree that rotations longer than 3 or 4 days are detrimental to the overall health of engineering staff and therefore the entire system. Follow the Sun: Follow the sun shifts are nice for distributed teams. These allow for engineers to schedule their “on-call” shifts only during their normal working office hours. As they end their day and go home, engineers in a different time zone can take over. And of course, there are many ways to customize shifts, especially for weekends when engineers need more flexibility. Engineers should be able to hand off the role to someone when personal conflicts arise. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Remediation of Incidents "
    }, {
    "id": 25,
    "url": "http://devopschampion.com/2020/03/05/2020-02-25-Building-Oncall-Rosters.html",
    "title": "Building On-call Rosters",
    "body": "2020/03/05 - Rosters establish a framework around who is on-call at any given point. A roster, or team, is made up of multiple engineers. Rosters can also contain multiple rotations. Resources: MS Learn: Improve Incident Response with Alerting in Azure Next steps: Building On-call Rotations "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}